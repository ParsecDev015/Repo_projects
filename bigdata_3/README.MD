# AnÃ¡lisis de ReseÃ±as de Productos con Apache Spark y Kafka

Este proyecto implementa un pipeline de procesamiento de datos en **Apache Spark**, utilizando **Kafka** para simular un flujo de datos en tiempo real.  
El objetivo es analizar reseÃ±as de productos de Amazon, clasificarlas por sentimiento y realizar un procesamiento tanto **batch** como **streaming**.

---

## Estructura del proyecto

bigdata_3/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ amazon.xlsx                 # Dataset base (fuente: Kaggle)
â”œâ”€â”€ resultados/
â”‚   â””â”€â”€ resenas_clasificadas/       # Resultados del procesamiento batch
â”œâ”€â”€ batch_processing.py             # Proceso batch (anÃ¡lisis estÃ¡tico)
â”œâ”€â”€ streaming_processing.py         # Proceso streaming (anÃ¡lisis en tiempo real)
â”œâ”€â”€ kafka_producer.py               # Emisor de mensajes a Kafka (productor)
â”œâ”€â”€ kafka_consumer.py               # Receptor de mensajes desde Kafka (consumidor)
â”œâ”€â”€ zookeeper_kafka_commands.txt    # Comandos para ejecutar Zookeeper, Kafka, Producer y Consumer
â”œâ”€â”€ requirements.txt                # Dependencias Python
â””â”€â”€ README.md                       # Este archivo

---

## Objetivos del proyecto

- Procesar y analizar reseÃ±as de productos de Amazon usando **Spark SQL** y **PySpark**.  
- Clasificar reseÃ±as como **positivas**, **negativas** o **neutras** segÃºn palabras clave.  
- Simular flujo de datos en tiempo real con **Kafka** y **Spark Streaming**.  
- Almacenar resultados en formato **CSV** para posterior anÃ¡lisis.

---

## Requisitos del entorno

- Ubuntu o Debian (se recomienda usar una **mÃ¡quina virtual** o **WSL**)
- Java 11+
- Apache Spark â‰¥ 3.5.1
- Apache Kafka â‰¥ 3.6.0
- Python 3.10+  
- pip

---

## InstalaciÃ³n del entorno Python

Desde el directorio raÃ­z del proyecto:

# Crear entorno virtual
python3 -m venv env
source env/bin/activate

# Instalar dependencias
pip install -r requirements.txt

---

## InstalaciÃ³n de Spark y configuraciÃ³n del conector Excel

1. Instala Apache Spark:
   sudo apt install openjdk-11-jdk -y
   cd /opt
   sudo wget https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz
   sudo tar -xvf spark-3.5.1-bin-hadoop3.tgz
   sudo mv spark-3.5.1-bin-hadoop3 /opt/spark

2. Configura las variables de entorno en ~/.bashrc o ~/.zshrc:
   export SPARK_HOME=/opt/spark
   export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
   export PYSPARK_PYTHON=python3

3. Descarga el conector de Excel compatible con Spark:
   sudo wget https://repo1.maven.org/maven2/com/crealytics/spark-excel_2.12/3.5.1_0.20.4/spark-excel_2.12-3.5.1_0.20.4.jar -P /opt/spark/jars/

4. Verifica que Spark lo reconozca correctamente:
   spark-shell --jars /opt/spark/jars/spark-excel_2.12-3.5.1_0.20.4.jar

---

## EjecuciÃ³n del procesamiento por lotes (Batch)

spark-submit --jars /opt/spark/jars/spark-excel_2.12-3.5.1_0.20.4.jar batch_processing.py

**Salida esperada:**
- Muestra el conteo de reseÃ±as por sentimiento.
- Genera los resultados en resultados/resenas_clasificadas/.

---

## EjecuciÃ³n del procesamiento en tiempo real (Streaming)

1. Inicia **Zookeeper**:
   sudo /opt/Kafka/bin/zookeeper-server-start.sh /opt/Kafka/config/zookeeper.properties

2. Inicia **Kafka**:
   sudo /opt/Kafka/bin/kafka-server-start.sh /opt/Kafka/config/server.properties

3. Crea el **topic**:
   /opt/Kafka/bin/kafka-topics.sh --create --topic reseÃ±as --bootstrap-server localhost:9092

4. Ejecuta el **Productor** (envÃ­a reseÃ±as al topic):
   python3 kafka_producer.py

5. Ejecuta el **Consumidor** (lee y analiza las reseÃ±as):
   spark-submit streaming_processing.py

---

## Resultados esperados

El procesamiento batch mostrarÃ¡ algo similar a:
+-----------+-----+
|sentimiento|count|
+-----------+-----+
|Positiva   |1430 |
|Negativa   |11   |
|Neutra     |24   |
+-----------+-----+

El procesamiento streaming imprimirÃ¡ los resultados en tiempo real conforme se produzcan mensajes desde Kafka.

---

## Licencia

Proyecto desarrollado como parte del curso de **Big Data y Procesamiento de Datos con Apache Spark**.  
Uso educativo y demostrativo.

---

## Autor

**William GÃ³mez**  
Desarrollador Full Stack / Entusiasta de Big Data  
Colombia ðŸ‡¨ðŸ‡´
